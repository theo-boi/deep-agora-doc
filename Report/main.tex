%
% Auteur initial inconnu.
% Modifié par olivier.ploton@univ-tours.fr le 21/09/2021
% À compiler avec pdflatex, bibilographie avec biber.
% Tous les fichiers doivent être encodés en UTF-8
% S'utilise en présence du fichier de bibiographie biblio.bib
% et des dossiers polytech/ (classe) et pic/ (images)
%

\documentclass{polytech/polytech}
\usepackage[strings]{underscore} % utile pour les _ dans la biblio (DOI)

% Fixe la présentation des listings
\lstset{
 columns=fixed,       
 numbers=left,                              
 numberstyle=\tiny\color{gray},             
 frame=single,                              
 backgroundcolor=\color[RGB]{255,255,255},  
 keywordstyle=\color[RGB]{40,40,255},       
 numberstyle=\footnotesize\color{darkgray}, 
 commentstyle=\it\color[RGB]{0,96,96},      
 stringstyle=\rmfamily\slshape\color[RGB]{128,0,0},  
 showstringspaces=false,                    
 language=C++
}

% Quelques formatages supplémentaires
\numberwithin{figure}{chapter}
\renewcommand\thesubsection{\thesection.\arabic{subsection}} 

% dossier des images
\graphicspath{{./pic/}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%
% Paramètres à fixer avant de commencer le document
%

\typereport{prddi5}       

\reportyear{2022-2023}

\title{Deep-Agora}
\subtitle{Incremental segmentation of images of old documents}
\reportlogo{polytech/polytech}
           
\student[di5]{Théo}{BOISSEAU}{theo.boisseau@etu.univ-tours.fr}

\academicsupervisor[di]{Jean-Yves}{RAMEL}{jean-yves.ramel@univ-tours.fr}

\industrialsupervisor{Rémi}{Jimenes}{remi.jimenes@univ-tours.fr}

\company[polytech/univ.png]{Centre d'études supérieures de la Renaissance}
    {59, rue Néricault Destouches\\37020 Tours, France}
    {cesr.univ-tours.fr}


\resume{%
La collaboration avec le CESR a donné naissance au logiciel Agora (issu du projet PaRADIIT) qui réalise simultanément l'analyse de la mise en page, la séparation texte/graphique et l'extraction de motifs.
L'objectif de ce projet est de faire une refonte complète d'Agora en utilisant une nouvelle approche orientée vers l'apprentissage profond.
}
\motcle{document}
\motcle{ancien}
\motcle{segmentation}
\motcle{sémantique}

             
\abstract{
The collaboration with the CESR resulted in the Agora software (from PaRADIIT Project) which simultaneously performs page layout analysis, text/graphics separation and pattern extraction.
The objective of this project is to do a complete overhaul of Agora using a new approach oriented towards deep learning.
}
\keyword{historical}
\keyword{document}
\keyword{semantic}
\keyword{segmentation}
\keyword{alto}

%
% Le poster. Il faut exactement 3 blocs.
%

\posterblock{Objectifs}{
\begin{itemize}
\item point 1
\item point 2 
\item point 3
\end{itemize}
}{pic/lifat.png}{}

\posterblock{Mise en œuvre}{
\begin{enumerate}
\item point 1
\item point 2 
\item point 3
\end{enumerate}
}{pic/lifat.png}{}

\posterblock{Résultats attendus}{
Voici du texte.
Voici du texte.
Voici du texte.
Voici du texte.
Voici du texte.
Voici du texte.
}{pic/lifat.png}{}


\newglossaryentry{agile}
{
	name=Agile,
	description={A set of software development practices designed to create and respond to changes as the project progresses}
}

\newglossaryentry{alto}
{
	name=ALTO,
	description={Analyzed Layout and Text Object is an open XML Schema to describe the layout and text of a document image}
}

\newglossaryentry{binarization}
{
	name=Binarization,
	description={Conversion of a picture to only black and white}
}




\newacronym{cesr}{CESR}{Centre for Advanced Renaissance Studies (fr. Centre d'études supérieures de la Renaissance)}
\newacronym{lifat}{LIFAT}{Laboratory of Fundamental and Applied Computer Science of Tours (fr. Laboratoire d'Informatique Fondamentale et Appliquée de Tours)}
\newacronym{rdp}{R\&D project}{Research \& Development Project}
\newacronym{moa}{fr. MOA}{Project/Product Owner (fr. Maître d'ouvrage)}
\newacronym{moe}{fr. MOE}{Project Manager / Scrum Master (fr. Maître d'œuvre)}
\newacronym{eocs}{EOCs}{elements of content}

\bibliography{biblio}
\makeglossaries

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}
             
\chapter{Introduction}
\section{Actors, issues and context}

The \gls{rdp} is the final work that a student engineer must complete to obtain his diploma.
It places the future engineer in a project situation by making him/her produce personal work and invites him/her to show initiative and maturity regarding a specific high-level problem.
The R\&D project is the subject of a dissertation and an oral presentation to a jury each semester, which lasts at least two days a week throughout the fifth year, i.e. 26 weeks.

This report aims to provide both the main document that everyone can read and all the technical and methodological elements.
It consists mainly of complete sections of the different documents produced, with the technical sections in the appendix.

The actors of this project are:
\begin{itemize}
\item the client, which here are \gls{cesr}, for which a contact is Rémi Jimenes, lecturer and researcher.
\item the \gls{moa}, who is Jean-Yves Ramel, professor of computer science, director of \gls{lifat} and academic tutor for this project.
He is responsible for representing the client by ensuring that the deadlines are met and that the product conforms.
\item the \gls{moe}, Théo Boisseau (that is me), an engineering student in his final year of study.
I decide on the technical means used to design the product by what was defined by the product owner.
\end{itemize}

The client expressed the need for easy-to-use interactive software so that its users, historians, could create their own scenarios for extracting \gls{eocs} from images of historical documents.
These historical documents are mainly Renaissance corpora, accessible from the CESR database, and contain mainly printed or manuscript text, illustrations and page ornaments.

The simplicity of creating extraction scenarios, their reuse and their adaptation to different documents are essential dimensions of the requirement.

However, this simplicity should not unduly compromise the reliability and performance of the software.
Image processing of historical documents is a particularly difficult task notably because of broken characters, stains, and poor paper quality.

To convert historical books into accessible digital libraries, LIFAT is developing image processing software that participates in a complete processing chain, including layout analysis, text/illustration separation (i.e. segmentation of elements of content), optical character recognition (i.e. OCR) and text transcription.
This project focuses on layout analysis and segmentation of elements of content of historical documents.

In recent years, the performance of some deep learning techniques has surpassed that of shallow methods established by experts on various image processing tasks.
As this progress has made many computer vision tools available, it now seems possible to meet this need with a completely new approach.


\section{Objectives}

This project aims to propose a new approach based on deep learning neural networks to solve this image processing problem.

To this end, the Deep-Agora R\&D project aims to build a prototype of an optimisation software capable of extracting textual and decorative elements of content from images of historical documents.

The user should not be responsible for training the models.
Therefore, several deep learning models can be created and trained to extract the elements of content required in the different use cases of the software.

Due to its nature as a prototype, the system needs to be composed of computational documents combining scripts and good documentation.
It must also provide access to training datasets and parameter storage files to reproduce the deep learning models created.

If the objective is achieved, the project can be continued and a scenario creation subsystem can be implemented to deploy the models created within it.


\section{Hypotheses}

For this project, we suppose there are no different typefaces in a single line of text.
However, there may be, so it will only be taken into account in future versions of this project.

The end users will only look for these elements of content:
\begin{itemize}
\item Blocks of texts
\item Printed and handwritten text-lines
\item Handwritten annotations
\item Initial capitals
\item Banners
\item Figures
\item Decorations
\end{itemize}

And will not look for more modern or scientific ornaments, such as:
\begin{itemize}
\item photographs
\item tables
\item graphics
\item formulas
\end{itemize}
Either way, new data sets should be used to train new neural network models.

Ideally, there should be a model for each element of content.
Otherwise, models can extract groups of elements of content, as few as possible.

Agora will continue to evolve over the years and new needs may arise.
Thus, documentation should be very good to ensure a successful takeover of the project.

A long period of time will be devoted to understanding the tools for deep learning, working on the data and training models.
If it wastes time on the project, the issue should be referred to the product owner.


\section{Methodological bases}

An Agile project management method is used to create learning loops to quickly gather and integrate feedback.
Therefore, the Scrum method should be preferred in which ideology is to:
\begin{itemize}
\item learn from experience
\item to self-organise and prioritise
\item to reflect on gains and losses to continuously improve
\end{itemize}

Contact with the product owner should be maintained as much as possible, as it helps to improve and learn considerably as the project progresses.

To this end, we set sprints with a fixed duration of 2 weeks, which means there are 5 sprints.
At least one deliverable, containing an e-mail, should be sent to the product owner at least every two weeks and preferably once a week.
During the implementation phase, a meeting to get feedback about the product should be scheduled at the end of each sprint.
The implementation phase starts on 4 January and ends with a final presentation around 3 April.

GitHub is used for configuration management, by creating two different repositories:
\begin{itemize}
\item Deep-Agora, which contains the source code of the project
\item Deep-Agora_DOC, which contains all the deliverables of the specification, analysis and modelling part of the semester 9
\end{itemize}

GitHub can also be used as a project management tool.
It offers a similar feature to Trello or Jira called Projects, an adaptable spreadsheet that can also integrate with my issues and pull requests on GitHub to help me plan and track my work efficiently.

Files of elements of content and their vignettes have an explicit naming convention to locate them by name.
It should indicate their encapsulation in other elements of content, hierarchically and separated by dots.
For example: 1.10.5 (<page>.<paragraph>.<line>) or 1.1 (<page>.<illustration>).



\chapter{General description}


\section{Project environment}

This project is part of a larger research project between CESR and LIFAT.
It is currently being carried out as part of a programme for the regional valorisation of old books (mainly dating from the Renaissance), namely the {\it Humanist Virtual Libraries} controlled by the CESR.

Within this programme, projects such as TypoRef which aims to identify specimens of similar typical characters, and BaTyr, a database of illustrations extracted, need software that meets the requirements of this project.

CESR does not have powerful computing machines capable of training deep neural networks, but it has several machines and a large amount of remote and on-premises storage.

Agora, the software developed and published ten years ago by LIFAT to process images of historical documents, is undergoing a complete overhaul in this project.
Its technologies need to be updated and, above all, its overhaul should meet the previously unattainable need for simplicity in scenario creation.

Therefore, no takeover of the existing system is planned, as it has to be completely redesigned.


\section{User characteristics}

End users of Deep-Agora are all historians of CESR.

They have a sufficient but moderate command of computer tools.
They often use them but need extensive training or solid documentation to use them in the case of advanced tools with complex functions.
They did not have a satisfactory experience with Agora, as its interface was too complex.
They do not need user access rights to use Agora.


\section{System features}

Users use this software to extract patterns.
For this purpose, they should:
\begin{itemize}
\item import a manifest (redirecting to a collection of images)
\item import images directly
\item import an existing scenario from their file system
\item define a scenario by defining iterative operations
\item run the scenario to view the extracted content items
\item export the results to an ALTO file
\item export the scenario to the file system, making it available for import.
\end{itemize}

In practice, from all the images in a collection, users select a typical one on which they build and test their scenarios to extract elements of content, label them, split them and iteratively merge them.
They can then save their scenarios and run them on other collections.
\\
\img{20221116DiagUseCase.png}{Use cases diagram}{width=\textwidth}\label{diagUC}

\section{General structure of the system}

Training deep neural network models is not a task intended for Deep-Agora end users.
This part of the project is to be carried out outside the software system, but within the environment, as the engineer's system.
It includes training data preparation of the datasets found on the internet and the deep learning laboratory where the neural network models are trained.

The software itself, Deep-Agora, simply receives trained neural network models and uses them as operations in the scenarios to extract elements of content.

Rules are another type of operation that can complete the scenarios with a more descriptive approach, to specifically label or merge elements.
This type of operation exists and will be part of Deep-Agora but is not the subject of this project.

The scenarios are managed by projects that deliver the images, provide them with available operations and save their results.

Image importation provides projects with usable images that are either directly provided or whose IIIF links allow them to be found from a manifest.

ALTO export converts the results of the scenarios to an ALTO XML data structure and saves them to ALTO files.

\img{20221116DiagComponent.png}{Component diagram}{width=\textwidth}\label{diagComp}



\chapter{State of the art / Technology watch}


\section{Existing system}

Ten years ago, LIFAT developed and published Agora to segment images of historical documents.

It works page by page by structuring them into elements of content called EOCs.
For each page, the resulting structure is a tree of EOCs, some of which are the parents of others, just as text boxes are the parents of text lines.
The input of the software is a list of images to be segmented, and the output is composed of an ALTO file and vignettes of EOCs.

The software is not automatic.
It uses a strong interaction with the user.
Among all the images in a document/collection, the expert (user) selects a typical one on which to build and test specific scenarios, which are then applied to all others.

The software was typically used for 2 cases:
\begin{itemize}
\item to extract all content items of a document. These items can then be used by the Retro software to recognise characters for transcription or .
\item to extract only figures such as initial capitals and banners. These figures can be stored in a database such as the CESR's Base de Typographie de la Renaissance (BaTyR) for renaissance typography, in order to study their history and thus that of their creators.
\end{itemize}

\paragraph{Definition of rules}

After the document image has been binarised with the possibility to choose between different binarisation algorithms, the black pixels are processed by scenarios.
Scenarios are sets of steps to build the tree of EOCs.
Each step is a configurable operation on the EOC tree.
Operations can consist of deleting or editing an element of content or of creating new ones by setting up rules.

These rules are either labelling rules or merger rules.
The labelling rules concern the position, size and neighbourhood of the content items.
They give a label to regions of the image.
For example, "if black pixels touch each other, they are connected components".
The merger rules concern only the neighbourhood of the content items. For example, "if the distance between two elements of content identified as characters is less than 3 pixels horizontally, they are merged into a single character".

\paragraph{Export to ALTO format}

ALTO (Analyzed Layout and Text Object), a standardised XML format, is used to save page layout information and OCR-recognized text from printed documents, including books, journals, and newspapers.

The Metadata Encoding and Transmission Standard (METS) provides metadata and structural information, while ALTO contains content and physical information.
This extension schema is intended to be used in conjunction with METS.
However, ALTO instances can also be used outside of METS as a stand-alone document.

There is one ALTO file per image.
ALTO files contain a style section where labels are listed.
The layout section contains what is on the page.
A page is divided into several regions:

\img{margins.jpg}{Regions of a page}{width=0.3\textwidth}\label{margins}

\paragraph{To change}

The software interface is complex and therefore too difficult for the end user to use.
Indeed, most of the rules require a lot of parameters and without being an expert user of AGORA, it can be very easy to get lost in the interface of complex scenarios.

Historical documents are very often damaged in some way: characters are broken, there is the presence of stains, or the paper is of poor quality.
As a result, binarisation is problematic as multiple algorithms have already been implemented that cannot satisfy sufficient efficiency on this type of document.

Deep learning could be a solution to binarisation and to this complexity of rules by avoiding all these configurations.
DL modules could then be used iteratively with simpler scenarios to extract specifically defined elements of content (EOCs).


\section{Segmentation using deep learning approaches}

It is not necessarily the case that deep learning is the best way to segment historical documents.
The best approach will depend on the specific characteristics of the documents and the goals of the segmentation.

Deep learning approaches, such as convolutional neural networks (CNNs) and recurrent neural networks (RNNs), can be effective for solving the segmentation problem of historical documents because they are able to learn complex patterns in the data and can handle large amounts of unstructured data.
This can be especially useful for segmenting historical documents, which may have various types of formatting and layout, and may use old or archaic language and writing conventions.

However, there are also several potential drawbacks to using deep learning for segmenting historical documents.
One disadvantage is that deep learning approaches often require a large amount of labelled training data, which may not be available for historical documents.
Additionally, deep learning approaches can be computationally intensive, which may be a concern for large datasets or for documents with complex layouts.

Overall, it is important to consider the specific characteristics of the historical documents being analyzed, as well as the goals of the segmentation, when deciding whether to use deep learning or another approach for segmenting these documents.
This is why other approaches, such as rule-based approaches may be used on demand by the Deep-Agora users.


\subsection{U-Net}

A U-Net is a type of convolutional neural network (CNN) that takes an image as input for image segmentation tasks, developed by Olaf Ronneberger, Philipp Fischer, and Thomas Brox in 2015.
It is designed to be efficient and easy to train, making it a popular choice for image segmentation tasks.
Its architecture is shaped like a U by consisting of an encoder network and a decoder network connected by a series of skip connections.

The encoder network processes the input image and extracts features from it.
It consists of a series of convolutional layers and max pooling layers.
The convolutional layers are responsible for learning features from the input image, while the max pooling layers downsize the feature maps and reduce the computational complexity of the network.

The U-Net uses skip connections, also known as shortcut connections, to connect the encoder and decoder networks.
These connections allow the U-Net to preserve information about the spatial context of the image, which is important for accurate image segmentation.

The decoder network uses the features from the encoder to generate a segmentation mask that indicates the boundaries between different regions of elements of content in the image.
It typically consists of a series of transposed convolutional layers.
These layers apply a transposed convolution operation to the input, which upsamples the feature maps and increases the spatial resolution.
The transposed convolutional layers use the skip connections from the encoder network to incorporate information about the spatial context of the image.

Finally, the output of the U-Net is a segmentation mask where each pixel value corresponds to the probability of a specific label, such as an element of content or a combination of elements of content.
The U-Net can then be trained to predict multiple labels, so different elements of content.


\subsection{Transfer learning}

Transfer learning can be a useful technique for deep learning historical document processing, particularly when there is a limited amount of labelled training data available.
By using a pre-trained model as a starting point and fine-tuning it on a new task or dataset, it is possible to leverage the knowledge learned from the pre-trained model to improve the performance of the model on the new task.

For example, if a pre-trained model has already been trained on a large dataset of modern documents, it may be possible to use transfer learning to fine-tune the model on a smaller dataset of historical documents.
This could allow the model to learn important features and patterns specific to historical documents, improving its ability to classify, transcribe, or extract information from them.

Transfer learning can also be useful for segmentation in historical documents, as it allows the model to leverage the knowledge learned from modern images and objects to identify similar features in historical documents.


The VGG (Visual Geometry Group) network is a CNN architecture that was initially developed for image classification tasks.
In the case of segmentation tasks, the VGG network can be used as a starting point for developing a CNN architecture that is specifically designed for segmentation.

For example, the VGG network could be modified and extended by adding additional convolutional and pooling layers, as well as skip connections, to better handle the complexity and spatial resolution of the segmentation task.
The modified VGG network could then be trained on a large dataset of labelled images, where the boundaries between different objects or regions of interest have been manually annotated.

In the case of transfer learning, the VGG network can be used as a pre-trained model to initialize the weights of a U-Net architecture that is being trained for a different task.

Therefore, in state-of-the-art DL frameworks for historical documents, models have a U-Net network in which the first layers are from a pre-trained VGG network.
The actual U-Net network can then be trained on a smaller dataset of historical documents.


\section{Frameworks for historical document processing}

Frameworks are platforms that provide a foundation for developing applications by providing generic functionality.
These functionalities can be selected or edited to provide a more specific application.

In our case, we need a framework for Historical Document Processing.
Its generic approach should allow to segment various elements of content and extract them from different documents.
For example, a framework using deep learning can help to train convolutional neural networks (CNN) or recurrent neural networks (RNN) on a dataset of labelled historical documents.
The operation using a trained network model could then be used to predict the regions of elements of content in new, unseen documents.

Frameworks using deep learning approaches can be very effective for solving the segmentation problem of historical documents, but they also require a large amount of labelled training data and may be computationally intensive.

Existing state-of-the-art training datasets for historical document processing have been created:

\img{20221110SpecsDataSets.png}{Sample of state-of-the-art datasets for historical document processing}{width=0.9\textwidth}\label{datasets}

For training a model, training data and configuration must be provided to the framework:
\begin{itemize}
\item Training data which can be a directory containing the images, a list of image filenames, or a path to a csv file.
\item Evaluation data which is used for validation, under the same format as training data.
\item A directory for model output.
\item A class file coding each region to segment.
\item Additional parameters for training, such as:
	\begin{itemize}
	\item Data augmentation to scale, rotate or editing the images
	\item Batch size
	\item Make patches by cropping image in smaller pieces
	\item Number of epochs to cycle trough data
	\item GPU
	\end{itemize}
\end{itemize}

After training, the inference of a model can be operated on a directory of input data, that returns a probability maps for each label


To choose the right framework, certain features should be taken into account.

It must explicitly handle historical documents.
Indeed, frameworks designed for historical document layout analysis usually use additional features than others, in order to take into account broken characters, stains, poor paper quality, and so on.

No binarisation algorithm should be used by the framework as preprocessing.
It has been revealed that binarisation algorithms were not efficient enough in previous versions of Agora, so they should avoided as a pre-processing requirement.

It must allow at least a segmentation at the line level.
Agora must extract text lines in text blocks.
As historical document analysis frameworks are usually designed for further OCR operations, they very often extract text lines.

It must allow decoration segmentation.
In historical documents are all types of visual EOC that are extremely valuable for their amount of historical information.

It should be compatible with handwritten text.
In the case of handwritten documents, one or more characters, words or even lines tend to touch each other and are treated as the same content items.
Therefore, the extraction of handwritten content items is impossible because too many of them touch each other.

The framework should learn from annotated masks.
Each training sample consists in an image of a document and its corresponding parts to be predicted.
Additionally, a text file encoding the RGB values of the classes needs to be provided.
In this case if we want the classes "background", "document" and "photograph" to be respectively classes 0, 1, and 2 we need to encode their color line-by-line:
\begin{table}[]
\definecolor{C}{HTML}{305496}
\begin{tabular}{|lll|}\hline
0 & 255 & 0 \\\hline
255 & 0 & 0 \\\hline
0 & 0  &255 \\\hline
\end{tabular}
\end{table}

The user expressed the need to have an ALTO file as output of Agora.
So it would be even better if the framework as an output well structured.

The Deep-Agora project should be continued.
Therefore, it would be better if the tools it uses were also pursued.

Good documentation of the tool would also help to shorten the developer's adaptation phase.

Because of these criteria, two good frameworks could be used: dhSegment and Kraken.

\paragraph{dhSegment}

Even if dhSegment seems to be the most suitable framework, its documentation is limited, especially for the last versions, and there is no ALTO conversion routine for its results.

\paragraph{Kraken}

Kraken offers great documentation and an ALTO conversion routine for its results.
However, it is not designed to segment visual elements of content and it seems that no good substitute for binarisation as a pre-processing operation has been found.



\chapter{Analysis and design}

\section{Analysis}


\subsection{Assumptions used}

State-of-the-art DL frameworks are not good enough to segment handwritten characters in images of historical documents.
If one appears during the development in the deep learning lab, it should be used in the project.

The elements of content to extract are:
\begin{itemize}
\item Blocks of texts
\item Printed and handwritten text-lines
\item Handwritten annotation
\item Initial capitals
\item Banners
\item Figures with (or without) their caption
\item Decorations
\end{itemize}
Ideally, there should be a model for each element of content.
Otherwise, models can extract groups of elements of content, as few as possible.

No other methods than grouping connected black pixels exist in frameworks to post-process the binary mask of predictions.
Because of that, the segmentation of characters is not possible for handwritten text and it has been removed from the list of elements of content.
If state-of-the-art frameworks actually enable that, then it could be a solution to segment characters in images of historical documents.
Appropriate new data sets with each character labelled individually should be used to train new neural network models.
ALTO files could also identify each character by a Glyph tag.

The DL frameworks do not use binarization algorithms as a pre-processing step.
If they actually do, the efficiency would not be as good.

Since the DL modules to develop cannot segment characters, ALTO is not the best format to export results.
This is because the ALTO format assumes that the bounding boxes are rectangular and either vertical or horizontal, which is not what the DL models return.
If it would be possible to segment characters, then it would not be that much of an issue.
The client expressed that they need an ALTO output, so it must be used.
A simpler version can be used if it wastes time on the project.


\subsection{Specifications}

\subsubsection{System}

The Deep-Agora software takes images of the CESR as inputs and output ALTO files and vignettes of the elements of content extracted.
To operate, it uses trained neural network models in operations.
Scenarios can also be saved and restored.
Neural network models must be trained outside of the software system: in the engineer's system.
Only after new models have been trained on prepared training datasets, they can be deployed in the software system.
\\
\img{20221117TasksInSystemEnvironmentLeg.png}{Tasks in the context of the system}{width=\textwidth}\label{diagSysEnv}

\paragraph{Import data}

Importing data consists of implementing a pipeline that acquires and prepares a dataset for each neural network model.
This way, downloaded datasets from the internet under different formats and containing different elements of content can be selected and merged to provide correctly structured ones.

\paragraph{Train new neural network models}

Training new neural network models consists of implementing a generic framework for historical document processing to segment images into targeted elements of content.
After acquiring the right prepared dataset, the model can be trained until it passes tests.

\paragraph{Operate neural network model}

Operating neural network model consists of implementing a module to operate a trained neural network model as an operation in scenarios.
For an image and an element of content to extract, the right model is restored to infer the image and returns a probability map for the element of content.
It can then be thresholded to segment the image.

\paragraph{Export results}

Exporting results consists of editing the outputs of scenarios to convert them into ALTO files and vignettes.
When multiple elements of content are extracted from the same image, they are first structured hierarchically.
Then, their name following the naming convention, their label and their coordinates can be altogether either written in an ALTO file or used to build vignettes extracted from the original image.

\paragraph{Manage scenarios}

Managing scenarios consists of easing the end users' projects.
Beyond importing images via manifests, the end users can manage their projects and refine the elements of content they want.
Scenarios can be saved or loaded, and run on a directory of images.


\subsubsection{Data}

The structure of a document refers to the organization of every element within it.
The organization of these elements in specific places of the document constitutes the layout of the document.
Detecting and extracting information is essential to get the geometry presented in a document.
A document may consist of several blocks of text such as title, paragraphs, main body text, text lines, graphics, tables and more.
Many datasets are publicly available to promote research that deals with the structure of documents.

The datasets to use should therefore contain labels such as layout, text-line and graphics.

The percentage of training and test data of the datasets should be defined during the project, accordingly to the amount of data available for selected elements of content and during the evaluation of the models.


\section{Proposed modelling}

\subsection{Data pipeline}

To train our models on appropriate data, we need to develop a series of processes that are used to extract, transform, and load data from one or more sources to a destination.

We first need to extract state-of-the-art training datasets by downloading them into the engineer's system.
Once the data has been extracted, we need to transform it to make it more suitable for the models.
In our case, this involves converting data formats.
Images must be converted to JPEG format and be of the same size.
Labels of EOCs written in PAGE XML files must be used to build mask images.
Mask images are RGB images in which each colour is associated with a different EOC.
These colours are associated with EOCs in a class file.

After the different datasets have been transformed, they are ready to be selected and merged into a single one.
This final dataset is then divided and saved in a training data folder and test, train and validation subfolders.
For each subfolder, there is an image folder and a label folder.

This pipeline is implemented in the Import data module of the engineer’s system, and this module interacts with the deep learning lab component.


\subsection{Multilabel semantic segmentation}

Regarding the class file in the previous subsection, it is important to note that in some cases, a pixel may belong to multiple classes or labels at the same time.
In this case, the pixel is referred to as a "multilabel pixel."
This case must be taken into account when developing a model capable of semantically segmenting several regions at once.
For example, a pixel in the image might belong to both the "text line" class and the "text block" class.

To represent the labels for each pixel in an image using dense encoding, we create a fixed-length vector for each pixel, with each element of the vector corresponding to a particular label.
The value of each element in the vector indicates the presence or absence of the corresponding label at that pixel.
If a pixel belongs to multiple labels at the same time, the corresponding elements in the vector would be set to 1.
For example, if a pixel contains both a line and a text block, the "text line" and "text block" elements of the vector would both be set to 1, while the "foot" element would be set to 0.

Therefore, in the class file, the RGB code of each colour is associated with an attribution code.
The class file could then look like this:
\begin{table}[]
\definecolor{C}{HTML}{305496}
\begin{tabular}{|llllll|}\hline
255 & 0 & 0 & 1 & 0 & 0\\\hline
0 & 255 & 0 & 0 & 1 & 0 \\\hline
0 & 0  & 255 & 0 & 0 & 1 \\\hline
0 & 255  &255 & 0 & 1 & 1 \\\hline
\end{tabular}
\end{table}


\subsection{Tree of EOCs}

The output of the neural network model is equivalent to an attribution probability for each pixel of the image.
Through the framework, we get a probability map of the image for each EOC.
In order to output results for the end user, we need to turn the regions into a hierarchically structured tree of EOCs.

Based on the class file and the types of EOCs, we know which regions are included in the others.
Then, the whole region of the text line is included in the region of the text block.
We can summarise this logic in this scheme:

\img{20231215EOCHierarchy.png}{Class diagram of the EOC hierarchy}{width=0.8\textwidth}\label{EOCdiag}

The objects of these classes can be structured following the exact same hierarchy.
For example, in the case where a pixel both belongs to a text line and a text block, we know the text line is included in the text block. In the case of a banner, it is considered as belonging to an illustration block.

Finally, this hierarchically structured tree can be used to name each element appropriately and to build the ALTO file.
An example of an ALTO file with the three types of EOCs "Block of text", "Text line" and "Banner" would look like this:

\begin{lstlisting}[language=XML]
<?xml version="1.0" encoding="UTF-8"?>
<alto xmlns="http://www.loc.gov/standards/alto/ns-v4#"
    xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
    xsi:schemaLocation="http://www.loc.gov/standards/alto/ns-v4#
     file:alto-4-3.xsd" SCHEMAVERSION=""
>
    <Description>
        <MeasurementUnit>pixel</MeasurementUnit>
        <sourceImageInformation>
            <fileName>[filename .e.g XXXXX.png]</fileName>
            <fileIdentifier fileIdentifierLocation="[path
             .e.g ../images]"/>
        </sourceImageInformation>
        <Processing ID="Agora"/>
    </Description>
    <Styles>
        <ParagraphStyle ID="[idString .e.g BLOCK]"/>
        <ParagraphStyle ID="[idString .e.g BANN]"/>
        <ParagraphStyle ID="[idString .e.g LINE]"/>
        <ParagraphStyle .../>
    </Styles>

    <Layout>
        <Page ID="agora.[uniqueString .e.g 0]" HEIGHT="[integer]"
         WIDTH="[integer]" PHYSICAL_IMG_NR="0">
            <TopMargin HEIGHT="[integer]" WIDTH="[integer]"
             HPOS="[integer]" VPOS="[integer]">
                ...
            </TopMargin>
            <LeftMargin ...>
                ...
            </LeftMargin>
            <RightMargin ...>
                ...
            </RightMargin>
            <BottomMargin ...>
                ...
            </BottomMargin>
            <PrintSpace ...>
                <ComposedBlock ID="agora.[uniqueString .e.g 0.0]"...>
                    <TextBlock ID="agora.[uniqueString .e.g 0.0.0]"
                     STYLEREFS="[idString]"
			  HEIGHT="[integer]" WIDTH="[integer]"
			  HPOS="[integer]" VPOS="[integer]">

                        <TextLine ID="agora.[uniqueString
                         .e.g 0.0.0.0]" .../>
                        ...
                    </TextBlock>
                    ...
                    <Illustration FILEID="agora.[uniqueString
			  .e.g 0.0.1.jpg]"
                     ID="agora.[uniqueString .e.g 0.0.1]" .../>
                    <TextBlock ID="agora.[uniqueString
			  .e.g 0.0.2]" ...>
                        ...
                    </TextBlock>
                    ...
                </ComposedBlock>
            </PrintSpace>
        </Page>
    </Layout>
</alto>

\end{lstlisting}



\chapter{Implementation}

We focused on developing deep learning models that will be employed in future software development.
We implemented the {\it deep_learning_lab} package that allows a data scientist to prepare data, train deep neural networks and use them for inference on images.
It resulted in the creation of deep neural networks, each specialising in the semantic segmentation of a specific element of content.
Therefore, we only worked on single labels, not multilabels.

\section{Tools and library used}

We decided to use the {\bf dhSegment-torch} framework.
dhSegment-torch is an external Deep-Learning framework for historical document processing cloned from the GitHub repository dhSegment-torch.
We chose it because of its generic approach that allows to segmenting various types of regions and extracting content from different types of documents.
Note that dhSegment-torch is currently under development and is the PyTorch version of the no longer developed dhSegment Tensorflow version.
However, only the Tensorflow version comes with documentation.

We used a Linux machine with a GPU as the processing time of training can be very long.
The machine must have CUDA installed and the rest of the dependencies can be installed by Conda from the {\it dependencies} directory at the root of the project.
The environment files have been edited to adapt to sm_86 CUDA architecture.

Besides dhSegment-torch, some very basic libraries are used such as {\it numpy} and {\it PIL} for image processing, {\it pandas} for CSV files processing and {\it xml} for XML files parsing.
The project's use of external dependencies and sub-modules, such as conda environment files and setup files, highlights the importance of managing dependencies and environment configurations to ensure consistency and reproducibility across different machines and environments.

In addition, integration tests are managed by {\it pytest}.
The use of tests for integration testing further enhances its reliability.
The tests ensure that the code behaves as expected under different scenarios and edge cases, providing confidence in the code's correctness and reducing the likelihood of errors and bugs.

The use of these dependencies also enables the project to leverage existing, well-tested, and reliable libraries and frameworks, reducing development time and enabling developers to focus on higher-level tasks.

\section{Implementation elements, technical choices}

The project structure consists of multiple working directories, including the primary focus {\it deep_learning}.
The {\it deep_learning} working directory provides a framework for data preparation, training, and inference for deep learning models.
The {\it deep_learning_lab} package provides tools for data preparation, including patching and selecting labels, as well as training and inference tools.
The dhSegment library provides the trainer and predictor classes for semantic segmentation.
The project dependencies are contained in the {\it dependencies/} folder.

By organizing the project into working directories for data science, future software development, and dependencies, the project enables a clear separation of concerns and promotes modularity, making it easier to maintain, test, and update the code.
Additionally, the separation of raw datasets into a dedicated folder and the use of sub-packages for data preparation and model training further enhances modularity and enables developers to use parts of the codebase in other projects.

The {\it deep_learning_lab} package included in the project follows software engineering principles by adopting a clear module structure and defining clear interfaces between different components.
The package also makes use of logging and configuration files, improving the project's maintainability and scalability.

Future work can include implementing multi-label support for data preparation and further customization of the trainer and predictor classes.

The {\it segmentation.ipynb} Jupyter Notebook acts as an application and demonstration of the use of the {\it deep_learning_lab} package.
First, it patches raw data sets into the {\bf Training data preparation} section and demonstrates how to use the Orchestrator class.
Next comes {\bf Deep learning lab} which shows how to use the Trainer class by training a model on a single label, and then how to use the Predictor class by using inference from the same model, on the same label and on test data.
Note that we did always used the same parameters for our models and did not tune them for better training: 
\begin{itemize}
\item batch size is 4,
\item number of epochs is 100,
\item learning rate is 1e-4,
\item decay rate for the learning rate is 0.9995,
\item evaluate the model on the validation set every 5 epochs,
\item 4 epochs to wait for improvement in validation loss before early stopping,
\item repeat 4 times the dataset,
\item 1e6 pixels in the images (size of the input layer)
\end{itemize}
Finally, the section {\bf Tests} shows an example of how to use the inference results and allows the user to test the performance of the model.

\section{Analysis of results, evaluation, quality}

We trained four models, one for each of the following labels: TextLine, TextRegion, Word, ImageRegion.

Here are the most useful metrics to know for our problem of semantic segmentation:
\begin{itemize}
\item The intersection-over-union (IoU) and mean intersection-over-union (mIoU) measures the overlap between the predicted segmentation and the ground truth segmentation by computing the ratio of the intersection between the two regions to their union.
It varies from 0 to 1, where 1 indicates a perfect overlap between the predicted and ground truth segmentations.
\item mIoU, on the other hand, calculates the average IoU across all classes in the dataset.
It provides a more comprehensive measure of the overall performance of the segmentation model.
\end{itemize}

\subsection{TextLine}

TextLine has the best trained model we have. Here is a good example of its performance, with the regions drawn on the image and the first 3 vignettes we extracted:

\img{TextLine/good/image.png}{Good TextLine predictions on an image}{width=0.7\textwidth}

\img{TextLine/good/0.png}{Predicted TextLine vignette 1}{width=1\textwidth}
\img{TextLine/good/1.png}{Predicted TextLine vignette 2}{width=1\textwidth}
\img{TextLine/good/2.png}{Predicted TextLine vignette 3}{width=1\textwidth}

And here is an example of bad performance:

\img{TextLine/bad/image.png}{Bad TextLine predictions on an image}{width=0.7\textwidth}

In order to get a better understanding of the quality of the model, here come the metrics we measured using Tensorboard.


We use this learning rate decay based on the exponential law:
\img{TextLine/metrics/learningrate.png}{Learning rate of TextLine model on training set}{width=0.7\textwidth}

We observe a fairly good loss curve on the training set:
\img{TextLine/metrics/trainloss.png}{Loss curve of TextLine model on training set}{width=0.7\textwidth}

We have the same metrics for the validation set:
\img{TextLine/metrics/valloss.png}{Loss curve of TextLine model on validation set}{width=0.7\textwidth}


However, we get much more information about the quality of training by looking at the following metrics on the validation set:
\img{TextLine/metrics/valiouLabel.png}{IoU metric for TextLine of TextLine model on the validation set}{width=0.7\textwidth}
\img{TextLine/metrics/valiouBack.png}{IoU metric for Background of TextLine model on the validation set}{width=0.7\textwidth}
\img{TextLine/metrics/valmiou.png}{mIoU metric of TextLine model on the validation set}{width=0.7\textwidth}
\img{TextLine/metrics/valprecision.png}{Precision metric of TextLine model on the validation set}{width=0.7\textwidth}


\subsection{TextRegion}

The TextRegion model always finds a region of text, but it is generally not fine enough and tends to create unnecessary or too few regions. Here is a good example of its performance, with the regions drawn on the image and the first 3 vignettes we extracted:

\img{TextRegion/good/image28.png}{Good TextRegion predictions on an image}{width=0.7\textwidth}

\img{TextRegion/good/1-28.png}{Predicted TextRegion vignette 1}{width=0.2\textwidth}
\img{TextRegion/good/2-28.png}{Predicted TextRegion vignette 2}{width=0.2\textwidth}

And here is an example of bad performance:

\img{TextRegion/bad/image3.png}{Bad TextRegion predictions on an image}{width=0.7\textwidth}


With the same learning rate decay, here are its loss curve on training and validation datasets:
\img{TextRegion/metrics/trainloss.png}{Loss curve of TextRegion model on training set}{width=0.7\textwidth}
\img{TextRegion/metrics/valloss.png}{Loss curve of TextRegion model on validation set}{width=0.7\textwidth}

Here are its other metrics on the validation set:
\img{TextRegion/metrics/valiouLabel.png}{IoU metric for TextRegion of TextRegion model on the validation set}{width=0.7\textwidth}
\img{TextRegion/metrics/valiouBack.png}{IoU metric for Background of TextRegion model on the validation set}{width=0.7\textwidth}
\img{TextRegion/metrics/valmiou.png}{mIoU metric of TextRegion model on the validation set}{width=0.7\textwidth}
\img{TextRegion/metrics/valprecision.png}{Precision metric of TextRegion model on the validation set}{width=0.7\textwidth}

As we can see, all the metrics vary a lot.
We did not have enough time to tune the models since we focused on the quality of the code.
A solution could have been to increase the number of epochs or the number of validation steps without increase to tolerate (the early stopping).
This way, the model could train more and maybe the loss on the validation set would start decreasing.

However, the result is unlikely to be much better, as the TextRegion masks in the training data actually often overlap.
It comes from annotation files that often make poor quality masks.
Here is an example:
\img{image.jpg}{Image of HS_107_192_object_132921 from "Baseline Competition - Complex Documents/Bohisto_Bozen_SetP" dataset}{width=0.4\textwidth}
\img{mask.png}{Mask of HS_107_192_object_132921 from "Baseline Competition - Complex Documents/Bohisto_Bozen_SetP" dataset}{width=0.4\textwidth}

A solution could be to ignore the annotation files that only contain 1 label TextRegion when patching the datasets.
They can easily be counted with the Orchestrator.validate method and verbose parameter greater or equal to 4.


\subsection{ImageRegion and Word}

Their models have very bad performances. They simply do not find any ImageRegion or Word in the test data.

The IoU metric for ImageRegion clearly shows that the ImageRegion model has not learned to recognise any of these elements:
\img{ImageRegion/metrics/valiouLabel.png}{IoU metric for ImageRegion of ImageRegion model on the validation set}{width=0.7\textwidth}

This bad performance comes from a lack of data: we only have 3 labelled ImageRegion in all of our datasets

Concerning the Word model, it is not as obvious:
\img{Word/metrics/valiouLabel.png}{IoU metric for Word of Word model on the validation set}{width=0.7\textwidth}

However, although we have about 40,000-word regions in all our datasets, they belong to only 135 images in total, of which 35 are from the pinkas dataset and 100 from the IEHHR dataset.
Therefore, we have a very bad diversity of our data for the Word regions.

In addition, the annotations from the IEHHR dataset, which constitutes most of the Word training data, makes poor quality masks that overlap:
\img{wordmask.png}{Mask of Volum_069_Registres_0004 from "IEHHR-XMLpages" dataset}{width=0.7\textwidth}



\chapter{Assessment and conclusion}

\section{Semester 9 review}

This report took more time than excepted to be completed.
Indeed, even after validation, it has been corrected and improved several times during holidays.

\paragraph{Tasks done}

The following tasks have been completed:
\begin{enumerate}
\item Study existing system and state-of-the-art approaches
\item Specify I/O
\item Specify datasets
\item Set up planning
\item Design diagrams
\item Write specification document
\item Review and validate specification deliverables
\end{enumerate}

\paragraph{Tasks in progress}

\begin{enumerate}
\item Set up the environment
\end{enumerate}

This task is particularly time-consuming if I want to configure my computer for using my GPU.
I have an NVIDIA GeForce RTX 3050 Laptop GPU, and I am experiencing trouble using it for development purposes.
If it persists, I will contact my supervisor and try to find a solution together.
The rest of the environment is operational.

\paragraph{Tasks to do}

\begin{enumerate}
\item Finish setting up the environment
\item Prepare dataset
\end{enumerate}

As soon as the environment is ready, the data pipeline will have to be started in order to soon try a pre-trained model of dhSegment.

\section{Semester 10 review}

%Bilan global $\Rightarrow$ respect du cahier des charges (fait / à faire)

The environment of the project has been hard to configure due to the incompatibility between the CUDA version of our GPU server and the requirements of the dgSegment-torch library.
In order to avoid this complication for the next users of the project, new environment files, solid documentation and an installation guide have been created.

We estimate the delay due to the environment, including research and migration, to be two weeks.
The progress of the project has been slowed down by two weeks due to illness.
The last two sprints, each lasting two weeks, were already planned as optional.

We have been able to develop a validated model of text line segmentation and analyse its performance.
However, we did not tune it.
Instead, we really focused on creating new packages for the project and created a modular project structure with a deep learning package.
As specified, we focused on the quality of the code and its documentation so that it is easily maintainable and scalable for future developers.

Finally, we offered the first functionality for output export by saving all the vignettes in a directory.
However, we did not go further in the development and therefore did not create functionality for building ALTO output files or HMIs (which was specified in the optional sprints).

\section{Quality assessment}

In terms of the quality of the code and the documentation, we are very satisfied with our achievements and we believe that the next developers working on this project will have all the elements to do a great job.

We also believe that we had good project management and managed to overcome the challenges we faced in our project.

However, we would have liked to have acquired and patched more diverse data, tuned our models and obtained better results for most of them, as only one has really satisfactory results.
Many datasets were not available, and we could have realised it earlier if we did not work as much on the specification part.

\section{Self-critical review}

We really enjoyed doing this project and thought it has been very formative.
It was a personal goal to work for a long time on a large deep learning project and this R\&D project made it possible.
We even insisted on getting this project from our supervisor and we are very grateful to have been able to work on it.
It reinforces our idea that we want to work as an engineer in the field of machine learning.

\begin{appendix}
\selectlanguage{english}



\chapter{Planning, project management}

\section{Specification phase}

\subsection{Evolution of the project}

\img{20221115PlanningSpecs.png}{Gantt Chart for planning of the specification phase}{width=\textwidth}\label{planSpecs}

\subsection{Job description}

\paragraph{Task 1: Study existing system and state-of-the-art approaches}
\begin{itemize}
    \item Start date: 22/09/2022
    \item End date: 19/01/2022
    \item Duration: 14 days
    \item
        Description: Study and present Agora's problems and what state-of-the-art generic frameworks enable historical document processing.
\end{itemize}

\paragraph{Task 2: Specify I/O}
\begin{itemize}
    \item Start date: 06/10/2022
    \item End date: 27/10/2022
    \item Duration: 22 days
    \item
        Description: Identify and interview Deep-Agora future end-users to gather information about the inputs and outputs of the software.
\end{itemize}

\paragraph{Task 3: Specify datasets}
\begin{itemize}
    \item Start date: 07/11/2022
    \item End date: 16/11/2022
    \item Duration: 10 days
    \item
        Description: Specify and identify the best state-of-the-art datasets for historical document processing.
\end{itemize}

\paragraph{Task 4: Set up planning}
\begin{itemize}
    \item Start date: 16/11/2022
    \item End date: 16/11/2022
    \item Duration: 1 day
    \item
        Description: Set up and review the initial planning of the project.
\end{itemize}

\paragraph{Task 5: Design diagrams}
\begin{itemize}
    \item Start date: 16/11/2022
    \item End date: 17/11/2022
    \item Duration: 2 days
    \item
        Description: Model via diagrams the system and the components of the project.
\end{itemize}

\paragraph{Task 6: Write Specification Document}
\begin{itemize}
    \item Start date: 17/11/2022
    \item End date: 01/12/2022
    \item Duration: 15 days
    \item
        Description: Design and write the specifications of the project with the completion of each part to put in the final report. The completeness of the document should allow simplifying the writing of the final report.
\end{itemize}

\paragraph{Task 7: Review and validate}
\begin{itemize}
    \item Start date: 02/12/2022
    \item End date: 13/12/2022
    \item Duration: 12 days
    \item
        Description: Review the specifications with the product owner, present them, complete the specification document and finish the final report.
\end{itemize}


\section{Implementation phase}

\subsection{Evolution of the project}

\img{20221115Planning.png}{Initial Gantt Chart for Agile planning of the implementation phase}{width=\textwidth}\label{planS10}

\subsection{Job description}

All these sprints aim to prioritise and propose different versions of Deep-Agora.

\paragraph{Minor release 1: Prototype DL module}
\begin{itemize}
    \item Start date: 04/01/2023
    \item End date: 19/01/2023
    \item Duration: 15 days
    \item
        Description: The first minor release is expected to offer a prototype deep learning module that prepares a training dataset and uses a pre-trained model that semantically segments the page layout and returns certain elements of content from the list above.
\end{itemize}

\paragraph{Minor release 2: New DL module for other EOCs}
\begin{itemize}
    \item Start date: 25/01/2023
    \item End date: 08/02/2023
    \item Duration: 14 days
    \item
        Description: After some corrections if necessary, the second minor release should offer another deep learning module targeting other elements of content. Most of them will most likely be trained on different data sets.
\end{itemize}

\paragraph{Minor release 3: Evaluation and tuning}
\begin{itemize}
    \item Start date: 09/02/2023
    \item End date: 22/02/2023
    \item Duration: 13 days
    \item
        Description: After some corrections if necessary, the third minor release should allow the previous models to be evaluated and tuned for better results.
\end{itemize}

\paragraph{Minor release 4: Output export}
\begin{itemize}
    \item Start date: 23/02/2023
    \item End date: 08/03/2023
    \item Duration: 13 days
    \item
        Description: A secondary fourth minor release should offer a solution to export the outputs of the previously trained models to vignettes and ALTO files.
\end{itemize}

\paragraph{Major release 1: Interface for end-user}
\begin{itemize}
    \item Start date: 09/03/2023
    \item End date: 22/03/2023
    \item Duration: 13 days
    \item
        Description: An optional major release is to develop the functionality for end-users to import images from manifests and manage scenarios so that they can refine the elements of content they want.
		      It should use all the modules developed in the previous sprints.
\end{itemize}

The two last sprints being optional, it is envisaged that they will only cover the remaining backlog from previous sprints.


Here are the final plannings of the project:
\img{20230404Planning1.png}{Final Gantt Chart for Agile planning of the implementation phase (part 1)}{width=\textwidth}\label{planS102}
\img{20230404Planning2.png}{Final Gantt Chart for Agile planning of the implementation phase (part 2)}{width=\textwidth}\label{planS102}


\chapter{Description of the interfaces}

\section{Hardware/software interfaces}

IIIF links require an Internet connection to send HTTP requests to online virtual libraries.

The machine on which the engineer's system deep learning lab will be run should have a GPU to process neural network training faster.

Data sets for training will be stored in the engineer's system.


\section{Human/machine interfaces}

The prototype should be made of computational documents combining scripts and good documentation, such as Jupyter Notebooks.

The HMI of the software should display at least 4 panels:
\begin{itemize}
\item Scenario: different operations in iterative order
\item Tree of EOC: elements of content organised structurally in a tree
\item Existing label: a list of extracted labels
\item Current image: a picture of the image being analysed
\end{itemize}

To build scenarios, operations can be accessed through different dedicated tabs.
A File tab is dedicated to the management of the user's project.
A project tab is dedicated to configuring it.
A scenario Tab is dedicated to clearing it or undoing the last operation performed.

The simplicity of the HIM to create scenarios, reuse them and adapt them to different documents is essential.


\section{Software/software interfaces}

To import images at the beginning of a project, databases are indirectly requested through the use of IIIF links.
IIIF links are URLs that return images in response to a standard HTTP or HTTPS request.
These links can redirect to internal or external networks.

Trained neural network models are implemented in Deep-Agora manually, by restoring their parameters from storage files.

In the engineer’s system, datasets are downloaded manually through websites, then transformed, and models are trained using a state-of-the-art generic framework for historical document processing.



\chapter{Specification}

\section{Functional specifications}

\subsection{Definition of module 1: Import training data}

\paragraph{Presentation:}
 
\begin{itemize}
    \item Name: Training data import module
    \item Summary: Implement a pipeline to prepare a training dataset for neural network model.
    \item Priority: Primary
    \item Interacting components:
    \begin{itemize}
        \item Deep learning lab component
        \item File system
        \item Datasets from the file system
        \item Engineer’s system
    \end{itemize}
\end{itemize}

\paragraph{Description:}
 
\begin{itemize}
    \item Inputs: State-of-the-art training datasets, with images and different labels of EOCs. 
    \item Preconditions: Available datasets are downloaded on local storage.
Their images are in different formats, such as TIFF or JPEG.
Their labels are stored under PAGE XML format.

    \item Outputs: A training data folder containing a class file and test, train and validation subfolders. Each of the subfolders contains an image folder and a label folder of mask images.
    \item Postconditions: The original datasets were selected and merged for the pipeline.
Dataset is split into test, train and validation folders.
Dataset themselves divided into pairs (images, labels) of images with the same name (excluding the extension).
The images in the image folder are in JPEG format.
The images in the label folder are in PNG format and are RGB masks of the regions to segment with a different colour for each EOC.
The class file is multi-label and has one row for each combination of EOC and each one has 3 RGB values and an attribution code.
\end{itemize}

\paragraph{Details:}

\begin{itemize}
    \item Prioritized features list: 
    \begin{enumerate}
        \item	Acquire datasets from the file system
        \item	Distribute them in different pipelines according to the EOCs targeted by the neural network models
        \item	Create a training data folder for each pipeline containing a class file and test, train and validation subfolders. For each subfolder, create an image folder and a label folder.
        \item	Convert the images to JPEG format and put them in the image folders
        \item	Convert PAGE XML label files to PNG mask images with colours associated with the class file and put them in the label folders
    \end{enumerate}
    \item Error handling and implementation: 
    \begin{itemize}
        \item	Some datasets are not available for download → Delete them from the list and evaluate again the feasibility of the model
    \end{itemize}
\end{itemize}


\subsection{Definition of module 2: Deep learning lab}

\paragraph{Presentation:}
 
\begin{itemize}
    \item Name: Deep learning lab component
    \item Summary: Computational documents that implement frameworks for historical document processing to train new neural network models to semantically segment images into targeted EOCs.
    \item Priority: Primary
    \item Interacting components:
    \begin{itemize}
        \item Training data import module
        \item Deep learning lab component
        \item File system
        \item Engineer’s system
    \end{itemize}
\end{itemize}

\paragraph{Description:}
 
\begin{itemize}
    \item Inputs: A training data folder containing a class file and test, train and validation subfolders. Each of the subfolders contains an image folder and a label folder of mask images.
    \item Preconditions: Subfolders are themselves divided into pairs (images, labels) of images with the same name (excluding the extension).
The images in the image folder are in JPEG format.
The images in the label folder are in PNG format and are RGB masks of the regions to segment with a different colour for each EOC.
The class file is multi-label and has one row for each combination of EOC and each one has 3 RGB values and an attribution code.

    \item Outputs: Neural network model objects stored in a file. If inference, probability map of each attribution code (combination of EOCs).
    \item Postconditions: Neural network model objects have been saved using the framework defined for loading and saving models in the non-functional specifications.
The probability map is pixel-wise labelled and its dimensions are image_height X image_length X number_of_attribution_codes.

\end{itemize}

\paragraph{Details:}

\begin{itemize}
    \item Prioritized features list: 
    \begin{enumerate}
        \item	For each neural network model, acquire the right pipeline according to the EOCs targeted
        \item	Declare the training parameters of the generalist deep learning framework.
        \item	Train the model
        \item	Validate it or reiterate from 2.
    \end{enumerate}
    \item Error handling and implementation: 
    \begin{itemize}
        \item	The dataset does not dispose of enough samples or is not balanced enough → communicate the error to the product owner and/or pass to another model 
    \end{itemize}
\end{itemize}


\subsection{Definition of module 3: Import images}

\paragraph{Presentation:}
 
\begin{itemize}
    \item Name: Import module
    \item Summary: Import images from a manifest.
    \item Priority: Optional
    \item Interacting components:
    \begin{itemize}
        \item File system
        \item Manifests
        \item Deep-Agora system
    \end{itemize}
\end{itemize}

\paragraph{Description:}
 
\begin{itemize}
    \item Inputs: A manifest file
    \item Preconditions: The manifest is in JSON format and contains page image URLs.
    \item Outputs: A page folder containing images
    \item Postconditions: The images in the page folder are in JPEG format and were downloaded from the manifest file.
\end{itemize}

\paragraph{Details:}

\begin{itemize}
    \item Prioritized features list: 
    \begin{enumerate}
        \item	Load manifest
        \item	For each page of a corpus in the manifest, extract its IIIF links and store them in a list
        \item	Download images from IIIF links
        \item	Convert them to JPEG format
        \item	Save them in the page folder
    \end{enumerate}
    \item Error handling and implementation: 
    \begin{itemize}
        \item	The IIIF link does not refer to an available image → pass to the next one
    \end{itemize}
\end{itemize}


\subsection{Definition of module 4: EOCs to tree}

\paragraph{Presentation:}
 
\begin{itemize}
    \item Name: Conversion of EOCs to tree
    \item Summary: Interpret the outputs of the neural network models to construct EOC overlap trees from the attribution codes.
    \item Priority: Secondary
    \item Interacting components:
    \begin{itemize}
        \item Deep learning  extractors
        \item Scenarios
        \item Deep-Agora system
    \end{itemize}
\end{itemize}

\paragraph{Description:}
 
\begin{itemize}
    \item Inputs: Probability map of each attribution code (combination of EOCs) and class file.
    \item Preconditions: The probability map is pixel-wise labelled and its dimensions are image_height X image_length X number_of_attribution_codes.
The class file is multi-label and has one row for each combination of EOC and each one has 3 RGB values and an attribution code.
    \item Outputs: An XML tree structuring EOC regions
    \item Postconditions: Each node is an EOC that is associated with its coordinates in the original image and a name according to the naming convention for vignettes.
\end{itemize}

\paragraph{Details:}

\begin{itemize}
    \item Prioritized features list: 
    \begin{enumerate}
        \item	Threshold the probability map to obtain the matrix of attribution codes for each pixel
        \item	Draw bounding boxes of each label
        \item	Structure EOC regions in an XML tree
        \item	Associate coordinates of the bounding boxes and a name to each node according to the naming convention for vignettes
    \end{enumerate}
    \item Error handling and implementation: 
    \begin{itemize}
        \item	
    \end{itemize}
\end{itemize}


\subsection{Definition of module 5: Export results}

\paragraph{Presentation:}
 
\begin{itemize}
    \item Name: Export module
    \item Summary: Export vignettes of elements of content and export their location and coordinates into ALTO files for each page.
    \item Priority: Optional
    \item Interacting components:
    \begin{itemize}
        \item Scenarios
        \item File system
        \item Deep-Agora system
    \end{itemize}
\end{itemize}

\paragraph{Description:}
 
\begin{itemize}
    \item Inputs: An XML tree structuring EOC regions
    \item Preconditions: Each node is an EOC that is associated with its coordinates in the original image and a name according to the naming convention for vignettes.
    \item Outputs: Vignettes of each element of content, structured in a results folder and an ALTO file.
    \item Postconditions: The vignettes in the vignette folder have their names hierarchically structured and they have the same content as the corresponding bounding boxes in the original image.
The ALTO file structures all the input elements of content in a tree so that each EOC is included in its overlapping EOC regions.
\end{itemize}

\paragraph{Details:}

\begin{itemize}
    \item Prioritized features list: 
    \begin{enumerate}
        \item	For each node of the XML tree, extract vignettes from the original image and save them in the vignette folder
        \item	Convert the XML tree to ALTO and save it in a file
    \end{enumerate}
    \item Error handling and implementation: 
    \begin{itemize}
        \item	
    \end{itemize}
\end{itemize}


\subsection{Definition of module 6: End user interface}

\paragraph{Presentation:}
 
\begin{itemize}
    \item Name: Manage scenarios
    \item Summary: End users can manage their project and refine the elements of content they want.
    \item Priority: Optional
    \item Interacting components:
    \begin{itemize}
        \item Project management component
        \item File system
        \item Deep-Agora system
    \end{itemize}
\end{itemize}

\paragraph{Description:}
 
\begin{itemize}
    \item Inputs: HMI instructions
    \item Preconditions: The other modules have been implemented.
    \item Outputs: -
    \item Postconditions: -
\end{itemize}

\paragraph{Details:}

\begin{itemize}
    \item Prioritized features list: 
    \begin{enumerate}
        \item	Enable end-user to organise their project
        \item	Enable saving and loading of scenarios
        \item	Enable scenarios to be run on multiple images
        \item	View results of scenarios on HMI panels
    \end{enumerate}
    \item Error handling and implementation: 
    \begin{itemize}
        \item	
    \end{itemize}
\end{itemize}


\section{Non-functional specifications}

\subsection{Development constraints and design}

The state-of-the-art framework for training the neural network model is dhSegment. The programming language is Python and the computational documents are made of Jupyter Notebooks. Jupyter Notebooks can be made of any IDE or Jupyter Lab, however, they use the Conda environment. IIIF links are requested by HTTP or HTTPS protocols.

\subsection{Functional and operational constraints}

\subsubsection{Performance}

There is no specific time limit for processing multiple images. However, the use of the HMI must be reactive as the construction of scenarios requires a great deal of experimentation by the user.

\subsubsection{Capabilities}

The software runs on a single computer. It takes 3 different types of neural network models: text lines, ornaments and figures. They are implemented manually and on demand. A model can process only one image at a time. The data from outside the system can consume significant storage.

The software itself should be light. However, memory constraints can become a risk for neural network models. This risk will be evaluated by making the prototype, and the right specifications will be detailed in the final report.

\subsubsection{Operating modes}

As a prototype, it can be started with a Jupyter Notebook file after starting a Jupyter server.
After implementing the HIM, it can be started with a python script. It remains on until the user closes the window.

\subsubsection{Controllability}

The data import should display data samples before and after pre-processing.
The deep learning lab should display the training parameters, the learning curves, the number of live epochs and a graphic of the learning curves at the end of the training.
During the prototype part, the results of the deep learning extractor should display the bounding boxes encapsulating the targeted elements of content on the image.
The ALTO export and the scenario management respectively display the ALTO file and the serialised scenario produced.

\subsubsection{Security}

The level of confidentiality of the system is non-existent: there is no user access control, and no keywords or passwords.

\subsubsection{Integrity}

ALTO files and serialised scenarios are not protected. The end user can save them wherever they want.
The software only connects to the Internet when a manifest requires it. There is no protection.


\subsection{Maintenance and development of the system}

Maintenance of the HIM is palliative (fr. curative), which means it should only be done punctually on specific issues.

Maintenance of the operations and scenarios is curative, which means they should be restored if there is an issue. It should also be perfective to improve efficiency and evolutive since new needs can appear.



\chapter{Developer's Workbook}

\section{Introduction}

The package-based structure of the {\it deep_learning_lab} module promotes code reusability and maintainability. The use of external dependencies, such as the dhSegment-torch framework, allows for faster development and reduces the amount of custom code that needs to be written. Overall, these practices contribute to a more streamlined and scalable project development process.

\section{Architectural diagrams and UML}

The {\it deep_learning_lab.data_preparation.patch} module defines the classes DataStructure, DataPatcher, AnnotationEncoder and MaskBuilder:
\begin{itemize}
\item Orchestrator is responsible for orchestrating the data preparation and patching process for a given set of datasets.
\item DataStructure is a description of a dataset in the file system.
\item DataPatcher allows to transform a dataset into a dhSegment framework compatible one containing the masks of the specified labels.
\item AnnotationEncoder inspects a directory of annotations files to select, extract, and encode labels and their features.
\item MaskBuilder creates masks for each labelled region in the image, given the annotations.
\end{itemize}

\img{20230403deeplearninglab.datapreparation.png}{Class diagram of the deep_learning_lab.data_preparation subpackage}{width=\textwidth}\label{dataPrep}

\section{Detailed descriptions of data used}

For training our deep neural networks, we used the 6 following datasets (some bring together several data sets):
\begin{itemize}
\item FCR
\item ESPOSALLES
\item ScriptNet
\item REID2019
\item Pinkas
\item ImageCLEF 2016
\end{itemize}

They constitute around 1,700 pairs (images, annotations).
Here are the results of our analysis using the {\it Orchestrator.validate} method:
\begin{table}[h]
    \centering
    \definecolor{C}{HTML}{305496}
    \begin{tabular}{|l|l|l|l|l|}\hline
        Dataset & TextLine & TextRegion & Word & ImageRegion \\\hline
        FCR\_500/data & 32177 & 1701 & - & - \\\hline
        ABP\_FirstTestCollection & 961 & 226 & - & - \\\hline
        Bohisto\_Bozen\_SetP & 815 & 152 & - & - \\\hline
        EPFL\_VTM\_FirstTestCollection & 252 & 38 & - & - \\\hline
        HUB\_Berlin\_Humboldt & 693 & 81 & - & - \\\hline
        NAF\_FirstTestCollection & 930 & 164 & - & - \\\hline
        StAM\_Marburg\_Grimm\_SetP & 857 & 214 & - & - \\\hline
        UCL\_Bentham\_SetP & 1024 & 191 & - & - \\\hline
        unibas\_e-Manuscripta & 848 & 96 & - & - \\\hline
        ABP\_FirstTestCollection & 4230 & 30 & - & - \\\hline
        Bohisto\_Bozen\_SetP & 910 & 26 & - & - \\\hline
        BHIC\_Akten & 2339 & 30 & - & - \\\hline
        EPFL\_VTM\_FirstTestCollection & 2790 & 28 & - & - \\\hline
        HUB\_Berlin\_Humboldt & 885 & 28 & - & - \\\hline
        NAF\_FirstTestCollection & 6147 & 29 & - & - \\\hline
        StAM\_Marburg\_Grimm\_SetP & 1064 & 30 & - & - \\\hline
        UCL\_Bentham\_SetP & 2294 & 31 & - & - \\\hline
        unibas\_e-Manuscripta & 1081 & 20 & - & - \\\hline
        pinkas\_dataset & 1013 & 175 & 13744 & - \\\hline
        IEHHR-XMLpages & 3070 & 968 & 31501 & - \\\hline
        ImageCLEF 2016 pages\_train\_jpg & 9645 & 765 & - & - \\\hline
        REID2019 & - & 454 & - & 3 \\\hline
    \end{tabular}
    \caption{Summary of label counts for each dataset}
    \label{tab:label-counts}
\end{table}


\section{Detailed descriptions of classes, modules, achievements}

\subsection{Data Preparation}
The raw datasets are expected to be placed in the {\it raw_datasets/} folder, located in the deep_learning working directory.
These datasets typically contain images and XML files containing their annotations.
These annotation files cannot be used directly to train the model because they must be converted to masks.
The {\it deep_learning_lab.data_preparation} package allows developers to select and use labels from their raw datasets to build masks.

To patch a dataset, the developer must specify the main directory, the image directory, and the annotation directory.
The {\it deep_learning_lab.data_preparation.orchestration} module provides default datasets that have been implemented inside the source code.
Additional datasets can either be added to the defaults datasets in the source code of the module or via the {\it Orchestrator.ingestDatasets} method.

The {\it deep_learning_lab.data_preparation.patch} module, which runs in the backend of the Orchestrator class, has not been validated for multi-labels since it misses the processing of one-hot encoding.

To extract annotations from data sources in a format other than PAGE, the DataPatcher class can be very easily extended to support other extraction methods by using the method {\it DataPatcher._securelyExtractUsing(self, extraction_fun)}.
Indeed, this method uses the extraction_fun method given as a parameter to parse XML files.
Here is a simple example of how to use it:
\begin{lstlisting}[language=Python]
def countLabelsFiles(self, names_labels) -> dict:
    """Counts the number of occurrences of specified label names in 
    the given XML page.

    Args:
        names_labels (iterable): An iterable of label names to
        				    search for.
    Returns:
        The number of occurrences of the specified label names in
	the XML page.
    
    """
    number_labels_name, _ = self._securelyExtractUsing(
        lambda page: xml.countTags(
            page,
            names_labels,
            self.namespaces_label
        )
    )
    return number_labels_name
\end{lstlisting}

\begin{lstlisting}[language=Python]
def extractCoordsLabels(self) -> dict:
    """Extracts the coordinates of specified label names from the
    XML page.

    Returns:
        A dictionary containing the size of the page and the
	coordinates of the specified labels.
        A list of any anomalies encountered during extraction.
    
    """
    shapes_label_name, anomalies = self._securelyExtractUsing(
        lambda page: {
            'size': self._getPageSize(page),
            'coords': xml.extractAttributesTag(
                'Coords',
                'points',
                page,
                self.codes_labels.keys(),
                self.namespaces_label
            )
        }
    )
    return shapes_label_name, anomalies
\end{lstlisting}

As you can see, an operation module called XMLParser used as {\it xml} has been created to easily parse annotation files.

\subsection{Training}
Before using the trainer provided by the dhSegment library, the developer must specify if they want to use a GPU and which one.
The {\it deep_learning_lab.gpu_setup} module allows the selection of a GPU/CPU in the backend.

The Trainer class from the {\it deep_learning_lab.model} module can then be instantiated with a specified set of labels to segment and the dataset to use.
The trainer can be configured with many parameters relating to the split of the validation and test sets or to the training of the model itself.

By default, the dataset to be used is {\it training_data} (results/'specified_labels'/training_data).
The model and tensorboard directories should be in the same location.
The model directory contains the best serialized models, and the tensorboard directory contains the logs of the metrics acquired during training.

\subsection{Inference}
The {\it deep_learning_lab.gpu_setup} module is also used for selection of a GPU/CPU in the backend.

The Predictor class from the {\it deep_learning_lab.model} module can be instantiated with a specified set of labels to segment.
The class can convert the input images from a CSV file into a folder in order to reuse the split test data from the training phase (it is only designed for developers).
It can draw regions around the elements of content in an image and cut out vignettes from it.
The output directory contains the vignettes and the output of the *`Predictor.start`* method returns additional data such as the original image on which the regions are drawn.

By default, the input data is {\it inference_data}, and the output directory is not specified.


\chapter{Installation document}

\section{Requirements}
To work in the deep_learning directory, it is recommended to have a Linux or WSL machine with a GPU, as the processing time can be very long. You also need to have Conda installed on your machine.

Check if you have a GPU and CUDA installed via the NVIDIA System Management Interface (NVIDIA-SMI) driver by entering in your terminal:
\begin{verbatim}
nvidia-smi
\end{verbatim}

\section{Installation}
The deep_learning_lab package uses the sub-module and framework dhSegment-torch. First, go to dependencies/ and clone the sub-module(s) as follows:
\begin{verbatim}
cd dependencies/
git submodule update --init --recursive
\end{verbatim}

The environment files environment.yml and setup.py of the dhSegment-torch framework have been edited to adapt to the sm_86 CUDA architecture of our machine. You can match your GPU name to your CUDA architecture on this website. To apply such changes, do as follows:
\begin{verbatim}
cp environment.yml dhSegment-torch/
cp setup.py dhSegment-torch/
\end{verbatim}

To install the package of the sub-module, go to dhSegment-torch/:
\begin{verbatim}
cd dhSegment-torch
\end{verbatim}

dhSegment will not work properly if the dependencies are not respected.
In particular, inaccurate dependencies may result in an inability to converge, even if no error is displayed.
Therefore, we highly recommend to create a dedicated environment as follows:
\begin{verbatim}
conda env create --name dhs --file environment.yml
source activate dhs
python setup.py install
\end{verbatim}

\chapter{Tests}

\section{Unit testing}

\subsection{Module 1: Import training data}

\begin{table}[]
\definecolor{C}{HTML}{305496}
\begin{tabular}{|l|l|}\hline
\color{C} IDENTIFICATION OF COMPONENT \\\hline
Request the right EOCs  \\\hline
\color{C} DESCRIPTION OF THE TEST\\\hline
Unknown EOCs are requested \\\hline
\color{C} EXPECTED RESULTS \\\hline
Raise an unknown parameter exception \\\hline
\color{C} OBTAINED RESULTS \\\hline
- \\\hline
\end{tabular}
\end{table}

\begin{table}[]
\definecolor{C}{HTML}{305496}
\begin{tabular}{|l|l|}\hline
\color{C} IDENTIFICATION OF COMPONENT \\\hline
Datasets are split correctly  \\\hline
\color{C} DESCRIPTION OF THE TEST\\\hline
There are X \% of the pairs (images, labels) in the train folder, Y \% in the test folder,\\ Z \% in the validation folder, and X+Y+Z=100 \% of the pairs from the datasets \\\hline
\color{C} EXPECTED RESULTS \\\hline
X+Y+Z=100 \% of the pairs from the datasets \\\hline
\color{C} OBTAINED RESULTS \\\hline
- \\\hline
\end{tabular}
\end{table}

\begin{table}[]
\definecolor{C}{HTML}{305496}
\begin{tabular}{|l|l|}\hline
\color{C} IDENTIFICATION OF COMPONENT \\\hline
Image formats are respected  \\\hline
\color{C} DESCRIPTION OF THE TEST\\\hline
For each image in the image subfolder  \\\hline
\color{C} EXPECTED RESULTS \\\hline
Each has the JPEG format \\\hline
\color{C} OBTAINED RESULTS \\\hline
- \\\hline
\end{tabular}
\end{table}

\begin{table}[]
\definecolor{C}{HTML}{305496}
\begin{tabular}{|l|l|}\hline
\color{C} IDENTIFICATION OF COMPONENT \\\hline
Masks have been built  \\\hline
\color{C} DESCRIPTION OF THE TEST\\\hline
For each image in the image subfolder \\\hline
\color{C} EXPECTED RESULTS \\\hline
Each has a PNG mask with the same name in the label subfolder \\\hline
\color{C} OBTAINED RESULTS \\\hline
- \\\hline
\end{tabular}
\end{table}

\begin{table}[]
\definecolor{C}{HTML}{305496}
\begin{tabular}{|l|l|}\hline
\color{C} IDENTIFICATION OF COMPONENT \\\hline
Masks were built correctly  \\\hline
\color{C} DESCRIPTION OF THE TEST\\\hline
For each PNG mask in the label subfolder  \\\hline
\color{C} EXPECTED RESULTS \\\hline
Each uses the colours from the class file \\\hline
\color{C} OBTAINED RESULTS \\\hline
- \\\hline
\end{tabular}
\end{table}

\subsection{Module 2: Deep learning lab}

\begin{table}[]
\definecolor{C}{HTML}{305496}
\begin{tabular}{|l|l|}\hline
\color{C} IDENTIFICATION OF COMPONENT \\\hline
Parameters are stored correctly  \\\hline
\color{C} DESCRIPTION OF THE TEST\\\hline
Training parameters configuration  \\\hline
\color{C} EXPECTED RESULTS \\\hline
Is stored in a JSON file \\\hline
\color{C} OBTAINED RESULTS \\\hline
- \\\hline
\end{tabular}
\end{table}

\begin{table}[]
\definecolor{C}{HTML}{305496}
\begin{tabular}{|l|l|}\hline
\color{C} IDENTIFICATION OF COMPONENT \\\hline
Masks use the class file definition  \\\hline
\color{C} DESCRIPTION OF THE TEST\\\hline
Predicted masks   \\\hline
\color{C} EXPECTED RESULTS \\\hline
Use the attribution codes of the class file \\\hline
\color{C} OBTAINED RESULTS \\\hline
- \\\hline
\end{tabular}
\end{table}

\begin{table}[]
\definecolor{C}{HTML}{305496}
\begin{tabular}{|l|l|}\hline
\color{C} IDENTIFICATION OF COMPONENT \\\hline
Each EOC had a predicted probability map  \\\hline
\color{C} DESCRIPTION OF THE TEST\\\hline
For any image in the test folder   \\\hline
\color{C} EXPECTED RESULTS \\\hline
A probability map is predicted by the model for each EOC it was trained to extract \\\hline
\color{C} OBTAINED RESULTS \\\hline
- \\\hline
\end{tabular}
\end{table}

\subsection{Module 3: Import images}

\begin{table}[]
\definecolor{C}{HTML}{305496}
\begin{tabular}{|l|l|}\hline
\color{C} IDENTIFICATION OF COMPONENT \\\hline
Images downloaded successfully  \\\hline
\color{C} DESCRIPTION OF THE TEST\\\hline
For each IIIF links    \\\hline
\color{C} EXPECTED RESULTS \\\hline
There is a JPEG image in the page folder \\\hline
\color{C} OBTAINED RESULTS \\\hline
- \\\hline
\end{tabular}
\end{table}

\subsection{Module 4: EOCs to tree}

\begin{table}[]
\definecolor{C}{HTML}{305496}
\begin{tabular}{|l|l|}\hline
\color{C} IDENTIFICATION OF COMPONENT \\\hline
Tree is structured hierarchically  \\\hline
\color{C} DESCRIPTION OF THE TEST\\\hline
For XML trees    \\\hline
\color{C} EXPECTED RESULTS \\\hline
Illustrations are outside any text block, and lines of text,\\ as well as annotations, are inside text blocks \\\hline
\color{C} OBTAINED RESULTS \\\hline
- \\\hline
\end{tabular}
\end{table}

\begin{table}[]
\definecolor{C}{HTML}{305496}
\begin{tabular}{|l|l|}\hline
\color{C} IDENTIFICATION OF COMPONENT \\\hline
Each node got all its information  \\\hline
\color{C} DESCRIPTION OF THE TEST\\\hline
Nodes of the XML tree     \\\hline
\color{C} EXPECTED RESULTS \\\hline
have a label, coordinates and a name following the naming convention. \\\hline
\color{C} OBTAINED RESULTS \\\hline
- \\\hline
\end{tabular}
\end{table}

\subsection{Module 5: Export results}

\begin{table}[]
\definecolor{C}{HTML}{305496}
\begin{tabular}{|l|l|}\hline
\color{C} IDENTIFICATION OF COMPONENT \\\hline
Each node of the tree was used for exportation  \\\hline
\color{C} DESCRIPTION OF THE TEST\\\hline
Nodes of the XML tree     \\\hline
\color{C} EXPECTED RESULTS \\\hline
have a corresponding vignette with the same name in the vignette folder. \\\hline
\color{C} OBTAINED RESULTS \\\hline
- \\\hline
\end{tabular}
\end{table}

\begin{table}[]
\definecolor{C}{HTML}{305496}
\begin{tabular}{|l|l|}\hline
\color{C} IDENTIFICATION OF COMPONENT \\\hline
ALTO file is complete  \\\hline
\color{C} DESCRIPTION OF THE TEST\\\hline
Tags of the ALTO file from the XML tree      \\\hline
\color{C} EXPECTED RESULTS \\\hline
have an id (name), coordinates and a style (label). \\\hline
\color{C} OBTAINED RESULTS \\\hline
- \\\hline
\end{tabular}
\end{table}

\begin{table}[]
\definecolor{C}{HTML}{305496}
\begin{tabular}{|l|l|}\hline
\color{C} IDENTIFICATION OF COMPONENT \\\hline
The structure is the same as that of the tree \\\hline
\color{C} DESCRIPTION OF THE TEST\\\hline
For each node of the XML tree that has a parent node, the corresponding tag of the ALTO file      \\\hline
\color{C} EXPECTED RESULTS \\\hline
has the corresponding parent tag. \\\hline
\color{C} OBTAINED RESULTS \\\hline
- \\\hline
\end{tabular}
\end{table}

\subsection{Module 6: End user interface}

\begin{table}[]
\definecolor{C}{HTML}{305496}
\begin{tabular}{|l|l|}\hline
\color{C} IDENTIFICATION OF COMPONENT \\\hline
User can select images and a current image  \\\hline
\color{C} DESCRIPTION OF THE TEST\\\hline
Click on Import image... button.\\ Select one or more images. \\\hline
\color{C} EXPECTED RESULTS \\\hline
First image is loaded and displayed as the Current image.\\ Other images are loaded in the background. \\\hline
\color{C} OBTAINED RESULTS \\\hline
- \\\hline
\end{tabular}
\end{table}

\begin{table}[]
\definecolor{C}{HTML}{305496}
\begin{tabular}{|l|l|}\hline
\color{C} IDENTIFICATION OF COMPONENT \\\hline
Operations can be selected by tabs  \\\hline
\color{C} DESCRIPTION OF THE TEST\\\hline
Click on Import image... button.\\ Select an operation. \\\hline
\color{C} EXPECTED RESULTS \\\hline
The current image is processed and displayed. \\\hline
\color{C} OBTAINED RESULTS \\\hline
- \\\hline
\end{tabular}
\end{table}

\begin{table}[]
\definecolor{C}{HTML}{305496}
\begin{tabular}{|l|l|}\hline
\color{C} IDENTIFICATION OF COMPONENT \\\hline
Operations can be added to from scenarios  \\\hline
\color{C} DESCRIPTION OF THE TEST\\\hline
Click on the tab of the operation.\\ Set up the operation. \\\hline
\color{C} EXPECTED RESULTS \\\hline
The operation is added to the list of the scenario. \\\hline
\color{C} OBTAINED RESULTS \\\hline
- \\\hline
\end{tabular}
\end{table}

\begin{table}[]
\definecolor{C}{HTML}{305496}
\begin{tabular}{|l|l|}\hline
\color{C} IDENTIFICATION OF COMPONENT \\\hline
Operations can be removed from scenarios  \\\hline
\color{C} DESCRIPTION OF THE TEST\\\hline
Click on an operation in the scenario.\\ Click Remove. \\\hline
\color{C} EXPECTED RESULTS \\\hline
The operation is removed from the list of the scenario. \\\hline
\color{C} OBTAINED RESULTS \\\hline
- \\\hline
\end{tabular}
\end{table}

\begin{table}[]
\definecolor{C}{HTML}{305496}
\begin{tabular}{|l|l|}\hline
\color{C} IDENTIFICATION OF COMPONENT \\\hline
Scenarios are saved in XML files  \\\hline
\color{C} DESCRIPTION OF THE TEST\\\hline
Click on the tab of the project.\\ Click on Save Scenario... \\\hline
\color{C} EXPECTED RESULTS \\\hline
The XML of the scenario contains the parameters of the operation. \\\hline
\color{C} OBTAINED RESULTS \\\hline
- \\\hline
\end{tabular}
\end{table}

\section{Integration testing}

\subsection{Module 1: Import training data}

\begin{itemize}
\item Data fits into memory: importing datasets from the file system does not throw a memory allocation error.
\item The module is run before the deep learning lab functions.
\end{itemize}

\subsection{Module 2: Deep learning lab}

\begin{itemize}
\item Regarding requested EOCs to Training data import module, a pipeline for these EOCs has been developed
\item Models’ variables are serialised in the file system
\end{itemize}

\subsection{Module 3: Import images}

\begin{itemize}
\item The module connects to the Internet.
\end{itemize}

\subsection{Module 4: EOCs to tree}

\begin{itemize}
\item 
\end{itemize}

\subsection{Module 5: Export results}

\begin{itemize}
\item ALTO file is stored in the file system.
\end{itemize}

\subsection{Module 6: End user interface}

\begin{itemize}
\item Operation interface can be implemented by Deep Learning Extractors and Rules
\item Calling the Import module fills the image folder of the project
\item Results are an XML tree handled by the Export module
\end{itemize}



\chapter{Glossary}
\begin{table}[]

\definecolor{C}{HTML}{305496}
\begin{tabular}{ll}
\textbf{A}	& \\
Agile	& A set of software development practices designed to create and respond to changes as the project progresses
ALTO	Analyzed Layout and Text Object is an open XML Schema to describe the layout and text of a document image\\
	
\textbf{B}	& \\
Binarization	& Conversion of a picture to only black and white\\
	
\textbf{C}	& \\
CESR	Centre d’études \\supérieures de la Renaissance\\
	
\textbf{D}	& \\
Deep Learning	& A type of artificial intelligence where the machine learns by itself using neural networks inspired by the human brain\\
	
\textbf{E}	& \\
EOC	& Element Of Content\\
	
\textbf{F}	& \\
Framework	& A platform that provides a foundation for developing applications\\
	
\textbf{I}	& \\
IIIF	& A standardised method of describing and delivering images over the web\\
	
\textbf{M}	& \\
Manifest	& A file containing metadata and URLs for a group of images\\
	
\textbf{P}	& \\
Pipeline	& A series of data processing steps\\
	
\textbf{S}	& \\
Sprint	& A time-bound effort, i.e. the duration is agreed and fixed in advance for each sprint\\
\end{tabular}
\end{table}



\chapter{Bibliography}

\begin{itemize}
\item \href{https://arxiv.org/abs/1804.10371}{S. Ares Oliveira, B.Seguin, and F. Kaplan, “dhSegment: A generic deep-learning approach for document segmentation”, in Frontiers in Handwriting Recognition (ICFHR), 2018 16th International Conference on, pp. 7-12, IEEE, 2018.}
\item \href{https://arxiv.org/abs/2203.08504}{K. Nikolaidou, M. Seuret, H. Mokayed and M. Liwicki, “A survey of historical document image datasets”, IJDAR 25, 305–338 (2022)}
\item \href{https://sites.google.com/site/paradiitproject/}{PARADIIT Project}
\item \href{https://github.com/theo-boi/deep-agora}{GitHub deep-agora}
\item \href{https://github.com/dhlab-epfl/dhSegment-torch}{GitHub dhSegment-torch}
\end{itemize}



\end{appendix}

\end{document}


